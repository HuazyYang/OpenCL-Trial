
- [AMD GCN GPU架构](#amd-gcn-gpu架构)
  - [处理器层次](#处理器层次)
    - [CU](#cu)
    - [存储器层次](#存储器层次)
      - [常量存储器](#常量存储器)
      - [局部存储器](#局部存储器)
      - [一级缓存](#一级缓存)
      - [私有寄存器](#私有寄存器)
- [NVDIA Kepler/Maxwell GPU 架构](#nvdia-keplermaxwell-gpu-架构)
  - [处理器层次架构](#处理器层次架构)
    - [Kepler架构](#kepler架构)
    - [Maxwell架构](#maxwell架构)
  - [SM](#sm)
    - [Kepler](#kepler)
    - [Maxwell](#maxwell)
  - [存储器层次](#存储器层次-1)
    - [全局存储器](#全局存储器)
    - [二级缓存](#二级缓存)
    - [常量存储器](#常量存储器-1)
    - [常量缓存](#常量缓存)
    - [局部存储器](#局部存储器-1)
    - [一级缓存](#一级缓存-1)
    - [私有存储器(寄存器)](#私有存储器寄存器)
- [OpenCL程序在NVIDIA GPU的映射](#opencl程序在nvidia-gpu的映射)
  - [设备](#设备)
    - [内核(kernel)占用](#内核kernel占用)
  - [SM](#sm-1)
    - [Warp](#warp)
    - [SIMT](#simt)
  - [CUDA Core](#cuda-core)
  - [存储模型映射](#存储模型映射)
    - [全局存储器(显存)](#全局存储器显存)
    - [纹理存储器](#纹理存储器)
    - [局部存储器](#局部存储器-2)
      - [<span id="NVBankConflict">存储体冲突</span>](#存储体冲突)
- [OpenCL程序在AMD GCN GPU上的映射](#opencl程序在amd-gcn-gpu上的映射)
  - [设备](#设备-1)
  - [CU](#cu-1)
  - [Wavefront](#wavefront)
  - [ALU](#alu)
  - [存储模型映射](#存储模型映射-1)
    - [全局存储器(显存)](#全局存储器显存-1)
    - [一级缓存](#一级缓存-2)
    - [二级缓存](#二级缓存-1)
    - [局部存储器](#局部存储器-3)
      - [存储体冲突](#存储体冲突-1)

# AMD GCN GPU架构
  ## 处理器层次
  * 模块化设计分为上个层次: 设备, CU和ALU.
  * GCN架构显卡Radeon HD7970, 具有32个CU, 每个CU 64个流核心, 故总共2048个核心.
  ```mermaid
    graph LR;
    A[模块化设计]
    B[设备]
    Bn[设备...]
    C[CU]
    Cn[32个CU...]
    D[流处理器]
    Dn[64个流处理器...]

    A --> B & Bn
    B --> C & Cn
    C --> D & Dn

  ```
 ### CU
  * 每个CU具有独立的局部存储器, 一级缓存和常量缓存;
  * 每个CU上的64个核心只能使用32个. 所有的32个核心共享统一的二级缓存, 二级缓存通过交叉总线(CrossBar)连接到显存控制器;
  * 每个CU上具有4个向量处理核心, 一个标量处理核心
  * 每个向量处理核心包含16个标量ALU.
  * 每个向量处理核心一次执行一条wavefront指令.
  * 标量核心用于计算分支, 常量缓存访问和其他wavefront为单位的操作.
  * 每个CU具有64K的局部存储器, 但每个工作组只可使用32K.
  * CU无超越函数的计算单元(与NVIDIA显卡相比)

  ```mermaid
  graph LR;
  A[CU]
  B[4个向量处理器]
  C[1个标量处理器]
  D[16个标量ALU]

  A --> B & C
  B --> D

  ```

 ### 存储器层次
  * CU对全局存储器和图形存储器的访问都通过二级缓存.
  * 二级缓存存在的目的是减少访问不对齐的影响, 增大带宽, 而不是减少延迟.
  #### 常量存储器
  * 常量存储器具有高性能, 独立的缓存, 常量缓存的大小限制为16KB.
  * 在AMD S10000 GPU上, 常量缓存的带宽为每CU每周期32B.
  #### 局部存储器
   * 只能被同一个工作组内的工作项读写.
   * 局部存储器大小为每CU具有64KB, 但运行在CU上的每个工作组最多只能使用32KB.
   * GCN局部存储器被划分为每CU具有32个存储体, 每个存储体4字节.
   * 局部存储器常用于通用并行计算时的数据共享和工作组内通信.
   * 在AMD S10000 GPU上, 局部存储器的最大带宽为每CU 128字节, 但大多数程序只能发挥64B带宽.
   * 局部存储体的使用需注意存储体冲突问题.
  #### 一级缓存
  * 每个CU上具有16KB的一级缓存, 它们被用于缓存对全局存储器的访问.
  #### 私有寄存器
  * GCN架构具有向量和标量寄存器.
    ##### 向量寄存器
    * 寄存器大小为每CU具有256KB, 每个寄存器32位, 故共有64KB个寄存器.
    * 如果数据大小是64位的, 则会使用相邻的两个寄存器.
    * GCN架构限制每个工作项最多可使用的向量寄存器数目位255.
    ##### 标量寄存器
    * 用于计算分支, 常量缓存访问和其他wavefront为单位的操作

# NVDIA Kepler/Maxwell GPU 架构
 ## 处理器层次架构
  * NVIDIA的GPU模块化为4个层次: 设备, 图形处理器簇(Graphic Processor Cluster, GPC), 流多处理器(Stream Multiprcessor, SM)和流处理器(Stream Processor, SP).
  * 从Femi时代开始, SP改称为Cuda Core.
  ### Kepler架构
  * 15个SMX, 每个SMX上192个Cuda Core. 总共2880个核心.
  * 每个SM独立的局部存储器, 一级缓存, 纹理缓存和常量缓存.
  * 二级缓存通过交叉总线(CrossBar)连接到显存控制器.
  ### Maxwell架构
  * 16个SM, 每个SM具有128个Cuda Core, 总共2048个核心.
  * 每个SM独立的局部存储器, 纹理缓存和常量缓存.
  * 二级缓存通过交叉总线(CrossBar)连接到显存控制器.

 ## SM
  * SM的角色类似AMD GCN的CU, 是模块化的核心. 通常认为SM才能称得上真正的核心, 因为只有它们才能够独立计算.
  * 在SM上, 主要的计算单元是Cuda Core.
  * 每个SM上有特殊函数(Special Function Unit, SFU)单元, 可进行超越函数和属性插值函数及其他特殊运算.
  * 从功能上说, CUDA Core只是GPU中的一条执行流水线, 它不停地执行流水线上指令. SP能够进行整型和单精度计算, 并拥有自己的指令指针(PC或IP寄存器), 因此能够自由分支.
  ### Kepler
   * 每个SM上有192个Cuda Core.
   * SM具有局部存储器, 其大小可配置为48KB, 32KB和16KB.
   * 局部存储器采用片上存储器, 速度极快.
  ### Maxwell
   * 每个SM上128个Cuda Core.
   * 128个Cuda Core分为4组, 每组一个warp调度单元和两个指令发射单元.
   * 局部存储器大小固定不可配置, 为64KB(Maxwell 1, 750Ti)或96KB(Maxwell 2, GTX980).

 ## 存储器层次
  ### 全局存储器
  * 索引空间内任何工作项均可读取, 并且在多个内核的调用将持久.
  * 延迟大, 用于缓存主机端的数据, 避免主机/设备之间的多次传输.
  * 在满足合并访问的条件下, 能够获得最大带宽.
  * 目前采用的GDDR5技术, 容量在2GB到12GB不等.
  ### 二级缓存
  * SM对全局存储器和图形存储器的访问都通过二级缓存.
  * 二级缓存对整个索引空间的工作项共享访问.
  * Kepler GPU具有1.5MB二级缓存, Maxwell GPU具有2M二级缓存.
  * 二级缓存的主要目的是减少访问不对齐的影响, 增大带宽, 而不是减少延迟, 因此其大小远小于对应的X86 CPU.
  ### 常量存储器
  * 用于加速常数的读取
  * 索引空间内所有工作项可见, 且在多个执行内核间持久.
  * 具有高性能, 独立的缓存.
  * 大小限制为64KB
  ### 常量缓存
  * Kepler上每个SM常数缓存的容量为48KB. **所有的纹理读取都会通过常数缓存**. 在Cuda C编程中, 访问`const restrict`限制的指针也会经过常数缓存.
  * 在Maxwell架构上, 所有对全局存储器的访问都会通过常数缓存.
  ### 局部存储器
  * 只能被同一个工作组内的工作项读写, 且只在工作项的生成期间内有效, 一旦工作组执行完成即不可用.
  * 使用时需要注意并行访问时的竞写问题.
  * Kepler GPU上, 局部存储器大小可配置为16KB, 32KB或48KB, 每个SM上局部存储器的最大带宽为每周期256B.
  * Maxwell上, 带宽为每周期每SM 128B.
  ### 一级缓存
  * Kepler GPU上, 一级缓存总大小为每SM 64KB. 每SM的一级缓存可配置为48KB, 32KB或16KB.
  * **一级缓存不缓存全局存储器的访问, 而只缓存私有寄存器溢出**.
  ### 私有存储器(寄存器)
  * 私有存储器通常映射为寄存器, 如果寄存器溢出, 则会保存到全局存储器并会被缓存到一级缓存中.
  * Kepler和Maxwell架构上, 每个SM拥有64KB个32位寄存器. 如果数据大小位64位, 则使用相邻的两个寄存器. 如果数据大小位128位, 则使用相邻的4个寄存器.


# OpenCL程序在NVIDIA GPU的映射

 ## 设备
  ### 内核(kernel)占用
   * Kepler最多允许32个内核同时占用, 并且允许多个主机线程同时占用.
   * 从Femi架构开始, 硬件允许多个内核同时执行在同一个GPU上. 这样在一个内核访问数据时, 另一个内核能够进行计算, 可有效提高设备利用率.

 ## SM
 * SM映射为OpenCL的CU. 一个工作组只能在一个CU上执行. 但CU允许执行多个工作组, 以允许多个工作项零代价切换**隐藏访问延迟**.
 * Kepler GPU最多可同时执行16个工作组, 而Maxwell间这个工作组提高到32.
 * 一个工作组中的所有工作项只能在一个SM上执行, SM再将工作组内的每个工作项映射到Cuda Core上.
 * SM在利用硬件多线程级并行的同时, 还能利用指令级并行;
 * 硬件的SM能够同时执行的工作项数目有限, 每个工作组支持的工作项数目也有限. Kepler和Maxwell架构中每个工作组最多支持1024个工作项, 但每个SM最多可同时执行2048个工作项.

  ### Warp
  * 工作组在CU上执行, 硬件会将其划分为更小的单元, CUDA和NVDIA OpenCL实现称其为**Warp**. 通常硬件采用相同的办法将工作组分割成Warp, Warp是对齐到32并且大小为32的线程组.
  * Kepler架构上的, 一个SM可以同时执行4个Warp, 这4个Warp可以来自同一个或不同的工作组. 4个Warp共享SM的指令发射, 执行, 调度单元. 这增大了SM实现的复杂性, 提高了延迟.
  * Maxwell架构上, SM被划分成4个更小的组件, 每个组件拥有自己独立的指令发射, 执行, 调度单元, 这简化了硬件设计, 降低了延迟, 提高了效率. 每个组件每次为一个Warp发射指令, **每次执行一条Warp指令**.
  * 实际执行中, Warp中的工作项是步调一致的, 严格串行, 因此无需同步. 利用这一点可实现许多高效的算法, 如CUDA中的warp vote函数. 因为Warp内工作项是严格串行的, 故当其内所有工作项都执行相同的代码代码时, 可有效地利用执行资源和指令流水.
  * Warp之间切换几乎没有消耗: 由于工作组的上下文在工作周期内被保存在硬件上, 不像CPU控制流切换时需要保存和重建上下文.
  * 分支执行影响: 如果一个Warp内的工作项执行N个不同分支路径, 那么SM就需要把每个分支路径的指令发射到每个CUDA Core上, 再由CUDA Core决定需不需要执行, 由于对执行流水的影响, **总执行时间大于各分支时间之和**. 需要注意的是, 分支只会再Warp内出现, 不同Warp的执行是独立的, 因此此时不存在分支, 故不会有性能损失.
  ### SIMT
  * 硬件利用一种SIMT(Single Instruction Multiply Thread)的执行模型管理, 调度众多线程. SIMT是SIMD的变种.
  * SIMT和SIMD的共同之处是多个元素上执行同一指令;
  * SIMT和SIMD的不同之处是SIMD宽度在代码代码编译时已经确定, 而SIMT实现的是依据硬件及软件条件自动决定自动向量宽度.

  ## CUDA Core
  * CUDA Core是标量处理器, 每次只执行工作项的一条指令, 映射到Warp的所有工作项共同执行一条向量指令.
  * 工作项拥有自己独立的寄存器状态和指令指针, 因此能够独立分支, 但由于**分支逻辑是共享的**, 因此对向量化性能会有极大的影响.

  ## 存储模型映射
  ### 全局存储器(显存)
  * 合并访问: 全局存储缓存降低了数据对齐的限制, 合并访问可以充分利用缓存, 提高带宽利用率.
  * 在Kepler和Maxwell架构上, **访问全局存储器以Warp为单位**, 因此是否满足合并访问要求也限定在Warp内.
  * 为了高效访问显存, 读取和存储必须对齐. 如果没有正确的对齐, 读写可能被硬件拆分为多次操作, 极大地影响效率.
  * 访问单元(Warp)内工作项的读写如果能够满足合并访问, 那么多次访问可以合并成一次完成, 提高访问效率.
  * NVIDIA Kepler和Maxwell架构由于拥有缓存, 其合并访问要求大为降低, 如果数据没有对齐时, 其利用率接近100%, 一级缓存线大小为128B. 缓存进一步降低了合并访问的要求, 减小了优化难度.
  * NVIDIA Kepler和Maxwell架构上, 访问同一地址会产生一次广播, 对性能没有损失.
  ### 纹理存储器
  * 纹理存储器通过常数缓存, 可以通过纹理对象, `ldg`或`const __restrict__`来利用常数缓存.
  ### 局部存储器
  #### <span id="NVBankConflict">存储体冲突</span>
  * 为了使一个访存单位(Kepler和Maxwell上是Warp)内的工作项能够并行访问, 局部存储器被组织成等于访存单元个数的存储体, 每个存储体拥有32位或64位的宽度, 相邻地址的数据存放到相邻的存储体中. 可以把局部存储器抽象为一个32列的矩阵, 矩阵的每个元素大小为4或8字节.
  * Kepler架构上, 每个存储体的宽度可配置为8字节, 此时局部存储器的带宽为每周期每SM 256字节. 为了发挥8字节带宽, 程序需要显示地使用8字节的模式访问共享存储器.
  * 如果一个访存单位(warp)中有一部分工作项访问属于同一个存储体, 不同地址的数据, 则会产生**存储体冲突**, 降低访问效率, *在冲突最严重的情况下, 速度可能比全局存储器还慢*. 需要指出的是: 如果多个工作项访问同一个存储体的不同字节, 不存在冲突, 但带宽会成比例下降.
  * 如果一个访存单元内的工作项访问同一地址, 会产生一次广播, 其速度反而没有下降.
  * 在不同的访存单元之间, 局部存储器毫不相关, 不存在冲突.

  如果寄存器使用过量, 可以把局部存储器作为寄存器使用, 缓存数据或状态信息.

# OpenCL程序在AMD GCN GPU上的映射
  ## 设备
  * AMD GCN架构具有两个硬件命令队列, 可同时向两个硬件命令队列提交计算任务. 这样一个内核访问数据时, 另外一个内核能够进行计算, 可有效提高设备利用率.

  ## CU
  * 一个工作组内调度到CU上执行, 但CU允许同时执行多个工作组, 以允许多个工作项零代价的切换隐藏访问延迟.
  * GCN GPU上可同时执行最多8个工作组, 一个工作组中的所有工作项只能在一个CU上执行, CU再将工作组内的每个工作项映射到ALU的一条卡槽上. CU再通过硬件多线程利用线程级并行同时, 还能利用指令级并行.
  * CU处理的每个工作组的执行进度信息(程序计数器, **局部存储器**, 寄存器)在其生存周期内被维护在硬件上. 由于工作组不能被换下, 因此需要其他的机制来隐藏工作组内的访问延迟.
  * 在GCN上, 每个工作组最多允许256个工作项, 每个CU最多允许2560个工作项同时执行.

 ## Wavefront
 * 工作组在CU上执行, 硬件会将其划分为更小的单元(Wavefront). Wavefront是指对齐到64且大小为64的工作项集合.
   分割时, 先分割0维, 再分割第1维, 最后分割第2维
 * GCN架构上, 一个GPU可同时执行4个Wavefront, 每个Wavefront被发射到一个ALU上执行.
 * 实际执行中, Wavefront中的工作项是步调一致的, 严格串行, 因此无需同步. 为了更好地利用硬件特性, OpenCL2.0标准加入了"子工作组"概念.
 * Wavefront切换几乎没有消耗: 切换Wavefront无需保存和重建上下文.
 * 分支执行影响: 如果Wavefront中的工作项走向N个分支路径, 那么需要CU将每个分支路径的指令(由CU上的一个标量处理核心)发送到ALU上, 再由ALU决定由那些槽执行, 由于对执行流水的影响, 总执行时间大于各分支时间之和. 要注意的时, 分支只会在Wavefront内出现, 不同Wavefront的执行是独立的, 因此此时不存在分支, 不会有性能损失.
 * 硬件利用SIMT的执行模型管理, 调度众多工作项. SIMT是SIMD的变种.
  
  ## ALU
  * ALU的每一条卡槽每次为每个工作项执行一条标量指令. 映射到一个ALU的Wavefront内的工作项共同执行一条向量(16个标量)指令.
  * 工作项拥有自己的独立的寄存器状态和指令指针, 能够独立分支, 但分支逻辑是共享的(CU上只有一个标量处理核心用于计算分支), 因此对向量化性能会有极大影响.

  ## 存储模型映射
  ### 全局存储器(显存)
  * 访问全局存储器的基本单位是1/4Wavefront, 因此是否满足合并访问的要求被限定在1/4Wavefront内, **1/4Wavefront 称之为访存单元**.
  * 为了能够高效访问显存, 读取和存储必须对齐, 如果没有正确对齐, 读写将被硬件拆分为多次操作, 极大地影响效率. 此外, 访存单元内工作项的读写操作如果能够满足合并访问(coalesced access), 那么多次访存操作会被合并成一次完成, 从而提高访问效率.
  * 从缓存的角度来讲, 合并访问也能够促进缓存的利用.
  * 在AMD GCN GPU上, **Wavefront访问同一个地址会产生最多的冲突**.

  ### 一级缓存
  * AMD GCN系列由于拥有一级缓存, 其合并访问要求大为降低. 如首地址没有对齐时, 其利用率接近100%. 一级缓存线长度为32字节. 缓存进一步降低了合并访问的要求, 减小了优化难度.

  ### 二级缓存
  * AMD GCN系列具有768KB的二级缓存, 二级缓存可用来加速全局存储器和图形存储器的读取.

  ### 局部存储器
  * GCN架构上, 每个CU具有64KB的局部寄存器, 但一个工作组最多能分配32KB.
  * 为了使得一个访存单位(1/4 Wavefront)内的工作项能够并行访问, 局部存储器被组织为16个存储体, 每个存储体32位宽度.
   #### 存储体冲突
   * 存储体冲突定义参考[NVIDIA GPU存储体冲突](#NV_Bank_Conflict).
   * GCN架构上，存储体冲突只发生在Half-Wavefront内, CU对局部存储的访问一次只能为1/4Wavefront的工作项服务.