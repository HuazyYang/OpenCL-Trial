# GPGPU异构并行计算性能优化

## 总体优化准则
  + **算法选择**. 选择算法的标准是看它们是否有很好的数据并行性，同时其计算访问比要尽量高，同时能够最大化并行.
  + **算法映射到硬件平台上**. 这主要包括工作项的组织和存储器的使用. 算法中的每个工作项的行为需要尽量保持一致, 如果分支很多,
    会严重降低GPU的运算效率各线程又选择不同的路径执行, 在CPU中, 即使有两个工作线程的行为高度不一致,也不会非常影响性能. 
    另外GPU需要许多工作项执行才能更好地隐藏延迟, 而CPU却需要少量工作项且每个工作项执行大量任务.
  + **并行度**. 在CPU中, 线程之间切换的开销是巨大的, 而基于GPU的并行运算设备中, 线程之间切换是非常廉价的, 设备通过线程之间切换
    隐藏内存访问延迟, 因此不鼓励运行具有很少线程的GPU算法．通常要求算法能够使用尽可能多的线程处理, 并且让每个线程承担尽可能多的计算.
  + **数据传输**. 寄存器(私有存储器)溢出问题的解决方法有:
    - 拆分代码以使用多个内核.
    - 使用局部存储器: 将某些变量放到局部存储器(尤其是小数组), 通过读写局部存储器减少使用寄存器.
    - 减少工作组的尺寸: 以Kepler GPU为例, 每个SM有64K个32位寄存器, 如果工作组中有1024个工作项, 那么内核最多只能使用64个32位寄存器, 而减少工作项个数位512, 则最多可使用512个寄存器.
  + **指令**. 注意条件分支语句. 展开循环. 使用inline函数减少调用. 优化指令使用, 尽量使用吞吐量达, 延迟低的指令, 使用硬件提供的快速指令或对应的编译器选项.

 ## 全局存储优化
  ### 合并访问
  合并访问, 就是尽量要求相邻的线程访问相邻的地址空间, 只用于加速**全局存储器**的访问. 尽量使得每次访问都在16字节对齐(`float4(vec4)`), 更好的利用缓存.

  ### 局部存储器
  每CU每周期的带宽: 64B(AMD GPU), 128/256B(NVDIA GPU). 延迟在几个, 十几个或者几十个时钟周期, 使用局部存储器访问取代全局存储器访问能极大节约带宽.

  内核内数组的声明, 软件呢开发人员可能希望它们保存在寄存器中, 但实际上通常保存在全局存储器中, 此时可显示地将其保存在局部存储器中, 以减少访问延迟.

  访问局部存储器需要注意存储一致性问题, 通过工作组内栅栏函数同步其写后读操作.

  ### 存储体冲突
  为了同时满足多线程的访问, 局部存储器以存储体为单位组织, 不同的存储体可同时向工作项提供服务. NVIDIA GPU计算能力2.0以下设备的访问的基本单元为half-warp(16), 局部存储器也划分位16个存储体, 故局部存储器一次也能满足half-warp的访问要求, 条件是half-wrap访问的数据在不同的存储体中; 计算能力版本2.0及以上的硬件, 基本访问单元为warp, 局部存储器划分为32个存储体; AMD GPU上, 访问单位为16个工作项(1/4 wavefront). 如果有两个或以上的工作项访问同一个存储体的不同地址, 就不能在一次存储体访问中满足访问单元要求, 这称为**存储体冲突**. 硬件通常将冲突的访问划分为几次访问, 这意味着性能随着冲突的大小直线下降.
  减少存储体冲突的方法:
  * 将冲突的相邻数据项改为间隔存放(每个工作项以工作组的工作项维度的模数处访问).

  ## 常量存储器优化
  常量存储器具有缓存, 一般而言其缓存命中时几十个周期可读. 目前AMD和NVIDIA的GPU要求一个访存单元
  (NVIDIA计算能力2.0以下硬件为half-warp, 其余为warp, 而AMD GPU为1/4wavefront)内的工作项访问相同的地址, 如果没有满足这个条件, 那么各个工作项的访问将被串行化.

  ## CUDA 纹理存储器优化
  纹理存储器不是缓存, 它只是利用纹理缓存而已.
  + GPU需要处理纹理时连续的二维图像, 因此纹理缓存也必须时两个维度上连续分布的.
  + 纹理缓存时只读的, 也不满足数据一致性.
  + 纹理缓存的主要功能是节省全局存储器的带宽和降低功耗, 而CPU的缓存则是为了实现较低的延迟.
  + 纹理可以实现对数据的特殊处理, 比如处理越界数据, 自动实现插值等.

  ## 寄存器及私有存储器优化
  私有存储器通常就是硬件的寄存器. GPU上的寄存器的数量是有限的.

  ## 工作组数量及大小
  ### 工作组大小
    通常工作组数量至少是CU数量的3倍, 如果有工作组同步的话, 要在4倍以上; 在NVIDIA GPU上, 工作组大小通常是warp大小的4倍(128)以上, 如果有工作组内同步的话, 可减小一些; 在AMD　GPU上, 工作组大小通常是wavefront的2倍(128); 此时基本可隐藏访问延迟.

  ### 工作组数量
    GPU对能够在内阁CU上同时执行的工作组数目做了限制, 比如NVIDIA Kepler GPU最多允许16个工作组(half-warp)同时在CU上执行.

  ## 占用率
  占用率是指CU上保留上下文的总工作项数量和最大允许数量的比例. 调整占用率的两种方式:
  + 调整工作组内工作项数目, 前提条件: 占用率不够大是因为工作组大小限制的; 如果占用率是由工作项使用的寄存器数量或工作组使用的局部寄存器数量限制的, 则另当别论;
  + 调整工作组内使用的局部存储器, 寄存器数量.

  ## 指令优化
    使用延迟低, 吞吐量大的指令替代延迟高或吞吐量小的指令.

  ## 分支优化
    SIMT执行模式: 以warp/wavefront为执行单位, 导致只要warp内有一个工作项执行分支, 那么整个warp**就需要执行该分支**, 因此warp内分支会导致性能下降.

    分支优化常用方法:
    + 将分支移到kernel外, 交给CPU处理;
    + 合并分支条件以减少分支路径数量;
    + 利用条件表达式和Blend表达式去除短分支;

  ## 数据传输优化
  主机和设备之间有独立的存器(除AMD unified memory model). GPU通过PCI-e在主机和设备之间交换数据.

